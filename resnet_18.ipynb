{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet-18 Baseline for Dyslexia-Oriented Character Classification\n",
        "\n",
        "This notebook implements a **baseline experiment** to evaluate ResNet-18 on character-level crops derived from YOLO annotations of a synthetic dyslexia handwriting dataset. The goal is to **establish a reliable reference point** (accuracy, macro-F1, confusion patterns) before comparing alternative architectures, and then **select the most optimal model** for the project.\n",
        "\n",
        "## Overview\n",
        "- **Task:** 3-class classification at the **character** level  \n",
        "  `0 = Normal`, `1 = Reversal`, `2 = Corrected`\n",
        "- **Data pipeline:** Image-level YOLO labels → per-bbox crops → square pad → resize → ResNet-18 head (3 classes)\n",
        "- **Safety constraints:** No flips/rotations (to preserve dyslexia-relevant geometry); light blur/erasing only\n",
        "- **Training setup:** ImageNet-initialized ResNet-18, AdamW, Cosine LR, label smoothing, AMP (CUDA), macro-F1 model selection\n",
        "- **Metrics:** Validation **Accuracy**, **macro-F1**, **Confusion Matrix**, per-class precision/recall\n",
        "\n",
        "## Why this baseline?\n",
        "- Provides a **fast, strong, and interpretable** starting point with a well-known backbone (ResNet-18).\n",
        "- Uses **macro-F1** to make model selection robust under possible class imbalance.\n",
        "- Confusion analysis highlights where the model under-performs (e.g., **Reversal → Normal**), informing targeted improvements.\n",
        "\n",
        "## Current result (validation)\n",
        "- macro-F1 ≈ **0.876**, Accuracy ≈ **0.873**\n",
        "- Dominant error mode: **Reversal** misclassified as **Normal**, suggesting the need for slightly wider context (e.g., small bbox inflation) or modest appearance jitter (no geometric changes)."
      ],
      "metadata": {
        "id": "0ZehCQuVHQt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torchvision\n",
        "print(\"cuda? ->\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(torch.cuda.get_device_name(0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBWf9J8xEAhm",
        "outputId": "607bea41-de35-4ea6-f982-7e00d3f17028"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda? -> True\n",
            "Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verifies whether PyTorch can see a CUDA-capable GPU and, if so, prints the device model (e.g., “Tesla T4”, “L4”, “A100”). This is a quick sanity check before starting any GPU-accelerated training."
      ],
      "metadata": {
        "id": "PdopDSzZHSon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "set_seed(42)\n",
        "\n",
        "# ---------- Config ----------\n",
        "IMG_SIZE = 224\n",
        "NUM_CLASSES = 3\n",
        "MEAN = [0.485, 0.456, 0.406]\n",
        "STD  = [0.229, 0.224, 0.225]\n",
        "SAVE_DIR = os.path.join(ROOT, \"runs_resnet18_yolocrops\")\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "def load_yolo_txt(txt_path: Path):\n",
        "    \"\"\"\n",
        "    YOLO format: class cx cy w h  (normalized)\n",
        "    \"\"\"\n",
        "    boxes = []\n",
        "    if not txt_path.exists():\n",
        "        return boxes\n",
        "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) != 5:\n",
        "                continue\n",
        "            c, cx, cy, w, h = parts\n",
        "            boxes.append((int(c), float(cx), float(cy), float(w), float(h)))\n",
        "    return boxes\n",
        "\n",
        "def norm_to_xyxy(box, W, H):\n",
        "    # normalized cx,cy,w,h -> pixel x1,y1,x2,y2\n",
        "    c, cx, cy, w, h = box\n",
        "    px = cx * W; py = cy * H\n",
        "    pw = w * W; ph = h * H\n",
        "    x1 = max(0, int(px - pw/2)); y1 = max(0, int(py - ph/2))\n",
        "    x2 = min(W-1, int(px + pw/2)); y2 = min(H-1, int(py + ph/2))\n",
        "    return c, x1, y1, x2, y2\n",
        "\n",
        "def pad_to_square(img: Image.Image, fill=0):\n",
        "    w, h = img.size\n",
        "    if w == h:\n",
        "        return img\n",
        "    if w > h:\n",
        "        pad = (0, (w-h)//2, 0, w-h-(w-h)//2)\n",
        "    else:\n",
        "        pad = ((h-w)//2, 0, h-w-(h-w)//2, 0)\n",
        "    return ImageOps.expand(img, border=pad, fill=fill)\n"
      ],
      "metadata": {
        "id": "Bql0TtzbtEty"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What this cell does\n",
        "- Imports core libraries (PyTorch/torchvision, PIL, scikit-learn metrics, NumPy) and seeds all RNGs for reproducibility.  \n",
        "- Defines dataset-wide configuration (input size, number of classes, ImageNet mean/std, output directory).  \n",
        "- Implements YOLO label parsing (`class cx cy w h` in normalized coordinates), conversion to pixel `x1,y1,x2,y2`, and a utility to pad rectangular crops to squares before resizing.\n",
        "\n",
        "## Key Design Choices\n",
        "- **CuDNN settings:** `benchmark=True` (faster convolutions for stable input shapes) and `deterministic=False` (accepts minor nondeterminism for higher throughput on Colab GPUs).  \n",
        "- **ImageNet normalization:** `MEAN/STD` matches ResNet-18 pretraining statistics, improving convergence stability.  \n",
        "- **Robust YOLO loader:** skips malformed lines and tolerates missing label files by returning an empty list—prevents crashes during dataset indexing.  \n",
        "- **Square padding:** uses `ImageOps.expand` to preserve aspect ratio before resizing to `IMG_SIZE`, avoiding geometric distortions that could blur dyslexia-relevant cues.\n",
        "\n",
        "## Notes & Pitfalls\n",
        "- For **strict determinism**, set `torch.backends.cudnn.deterministic=True` and `torch.backends.cudnn.benchmark=False` (training will be slower).  \n",
        "- Label IDs are assumed to be `{0,1,2}` → `{Normal, Reversal, Corrected}`; ensure your dataset follows this convention.  \n",
        "- Non-RGB inputs are converted to RGB later; remove alpha channels if present.  \n",
        "- If black padding biases the model (tiny crops with large borders), consider reflect/replicate padding, mean-color padding, or slightly **inflating** the bbox before cropping (introduced in later cells).\n"
      ],
      "metadata": {
        "id": "N3JqPEh9InDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class YOLOCropDataset(Dataset):\n",
        "    \"\"\"\n",
        "    images_dir: .../images/train\n",
        "    labels_dir: .../labels/train\n",
        "    Her görüntüdeki TUM bbox'ları ayrı örnek olarak döndürür.\n",
        "    \"\"\"\n",
        "    def __init__(self, images_dir: str, labels_dir: str, img_size=224, split=\"train\"):\n",
        "        self.img_root = Path(images_dir)\n",
        "        self.lbl_root = Path(labels_dir)\n",
        "        self.items: List[Tuple[str, int, Tuple[int,int,int,int]]] = []\n",
        "        self.img_size = img_size\n",
        "        self.split = split\n",
        "\n",
        "        img_paths = sorted(glob.glob(str(self.img_root / \"*.*\")))\n",
        "        for ip in img_paths:\n",
        "            ipath = Path(ip)\n",
        "            stem = ipath.stem\n",
        "            txt = self.lbl_root / f\"{stem}.txt\"\n",
        "            boxes = load_yolo_txt(txt)\n",
        "            if not boxes:\n",
        "                continue\n",
        "            # boyut almak için bir kez aç\n",
        "            with Image.open(ip) as _im:\n",
        "                _im = _im.convert(\"RGB\")\n",
        "                W, H = _im.size\n",
        "            for b in boxes:\n",
        "                c, x1, y1, x2, y2 = norm_to_xyxy(b, W, H)\n",
        "                # kutu min boyut filtresi: çok küçükse atla (isteğe bağlı)\n",
        "                if (x2 - x1) < 3 or (y2 - y1) < 3:\n",
        "                    continue\n",
        "                self.items.append((ip, c, (x1,y1,x2,y2)))\n",
        "\n",
        "        # Augmentations — flip/rotate YOK\n",
        "        if split == \"train\":\n",
        "            self.tf = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.1),\n",
        "                transforms.Normalize(mean=MEAN, std=STD),\n",
        "                transforms.RandomErasing(p=0.05, scale=(0.02, 0.06), ratio=(0.3, 3.3), value='random'),\n",
        "            ])\n",
        "        else:\n",
        "            self.tf = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=MEAN, std=STD),\n",
        "            ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ipath, c, (x1,y1,x2,y2) = self.items[idx]\n",
        "        with Image.open(ipath) as img:\n",
        "            img = img.convert(\"RGB\")\n",
        "            crop = img.crop((x1,y1,x2,y2))\n",
        "        crop = pad_to_square(crop, fill=0).resize((self.img_size,self.img_size), Image.BILINEAR)\n",
        "        x = self.tf(crop)\n",
        "        y = torch.tensor(c, dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "def make_loaders(root: str, batch_size=128, num_workers=2):\n",
        "    images_train = os.path.join(root, \"images/train\")\n",
        "    labels_train = os.path.join(root, \"labels/train\")\n",
        "    images_val   = os.path.join(root, \"images/val\")\n",
        "    labels_val   = os.path.join(root, \"labels/val\")\n",
        "\n",
        "    ds_tr = YOLOCropDataset(images_train, labels_train, img_size=IMG_SIZE, split=\"train\")\n",
        "    ds_va = YOLOCropDataset(images_val,   labels_val,   img_size=IMG_SIZE, split=\"val\")\n",
        "\n",
        "    # sınıf dengesizliği için weights (opsiyonel)\n",
        "    labels = [y for _,y,_ in ds_tr.items]\n",
        "    if len(labels) > 0:\n",
        "        counts = np.bincount(labels, minlength=NUM_CLASSES).astype(np.float32)\n",
        "        inv = (1.0 / np.maximum(counts, 1))\n",
        "        weights = [inv[y] for y in labels]\n",
        "        sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
        "        dl_tr = DataLoader(ds_tr, batch_size=batch_size, sampler=sampler, num_workers=num_workers, pin_memory=True)\n",
        "    else:\n",
        "        dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "    dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "    return ds_tr, ds_va, dl_tr, dl_va\n",
        "\n",
        "ds_tr, ds_va, dl_tr, dl_va = make_loaders(ROOT, batch_size=128, num_workers=2)\n",
        "len(ds_tr), len(ds_va)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiI73suftHuE",
        "outputId": "0b6e2bb2-8f4b-44ac-db9d-8dddf72faf1d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(112906, 52126)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What this cell does\n",
        "- Defines `YOLOCropDataset`, a dataset that **reads YOLO-formatted labels** for each image, converts normalized boxes to pixel coordinates, and **returns per-character crops** with class IDs (`0=Normal, 1=Reversal, 2=Corrected`).\n",
        "- Applies **safe augmentations** for the dyslexia setting (no flips/rotations), pads crops to squares, and resizes them to `IMG_SIZE`.\n",
        "- Provides `make_loaders(...)` to build **train/val datasets and dataloaders**, including an optional **class-balanced sampler** to mitigate label imbalance.\n",
        "\n",
        "\n",
        "## Key Design Choices\n",
        "- **Per-bbox sampling:** Each bounding box becomes a training sample. This matches label granularity (letter-level) and avoids ambiguity from page/line-level labels.\n",
        "- **No geometric flips/rotations:** Preserves dyslexia-relevant cues (e.g., reversals). Augmentations are limited to light blur and erasing to improve robustness without altering orientation.\n",
        "- **Square padding + resize:** `pad_to_square → resize(IMG_SIZE)` keeps character geometry stable and avoids aspect-ratio distortion.\n",
        "- **Tiny-box filter:** Discards extremely small boxes (`<3 px` in width/height) to reduce noisy crops that can harm learning.\n",
        "- **Class rebalancing (optional):** `WeightedRandomSampler` uses inverse class frequency from `np.bincount` to reduce bias toward dominant classes in minibatches.\n",
        "\n",
        "## Notes & Pitfalls\n",
        "- **Label presence:** If an image has no valid boxes (missing/empty `.txt` or all tiny), it contributes no samples; this is intentional to prevent crashes during indexing.\n",
        "- **Transforms:** Train split uses `GaussianBlur` and `RandomErasing` after normalization; val split is deterministic (no stochastic transforms).\n",
        "- **I/O considerations:** Opening images on-the-fly may bottleneck training. For large datasets, consider pre-cropping to an `ImageFolder` structure under `/content/` for faster epochs.\n",
        "- **DataLoader settings:** `num_workers=2` is conservative. On Colab GPUs, try `4–8`, set `pin_memory=True`, and (optionally) `persistent_workers=True, prefetch_factor=4` to increase throughput.\n",
        "- **Memory format (later):** When moving batches/models to GPU, using `memory_format=torch.channels_last` can provide minor speedups with AMP.\n",
        "- **Reproducibility:** Global seeding is handled elsewhere; note that stochastic ops (e.g., RandomErasing) introduce minor variability across runs.\n",
        "\n",
        "## Outputs\n",
        "- `ds_tr, ds_va` are `Dataset` objects with per-bbox samples.\n",
        "- `dl_tr` may use a class-balanced sampler (if labels exist); `dl_va` is a standard sequential loader.\n",
        "- `len(ds_tr), len(ds_va)` prints the **number of cropped samples** (not the number of images).\n",
        "\n"
      ],
      "metadata": {
        "id": "wLdChVWTIywO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_resnet18(num_classes=3, pretrained=True):\n",
        "    m = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None)\n",
        "    m.fc = nn.Sequential(nn.Dropout(0.2), nn.Linear(m.fc.in_features, num_classes))\n",
        "    return m\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    preds, gts = [], []\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device, non_blocking=True)\n",
        "        yb = yb.to(device, non_blocking=True)\n",
        "        with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
        "            logits = model(xb)\n",
        "        pr = logits.argmax(1)\n",
        "        preds.append(pr.cpu()); gts.append(yb.cpu())\n",
        "    preds = torch.cat(preds).numpy()\n",
        "    gts   = torch.cat(gts).numpy()\n",
        "    acc = accuracy_score(gts, preds)\n",
        "    f1  = f1_score(gts, preds, average=\"macro\")\n",
        "    cm  = confusion_matrix(gts, preds, labels=[0,1,2])\n",
        "    rep = classification_report(gts, preds, labels=[0,1,2], target_names=[\"Normal\",\"Reversal\",\"Corrected\"])\n",
        "    return acc, f1, cm, rep\n",
        "\n",
        "def train_resnet18(\n",
        "    epochs=20, lr=3e-4, weight_decay=1e-4, batch_size=128, out_dir=SAVE_DIR\n",
        "):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    ds_tr, ds_va, dl_tr, dl_va = make_loaders(ROOT, batch_size=batch_size, num_workers=2)\n",
        "    print(f\"Train samples (crops): {len(ds_tr)} | Val: {len(ds_va)}\")\n",
        "\n",
        "    model = build_resnet18(NUM_CLASSES, pretrained=True).to(device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n",
        "    crit = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n",
        "\n",
        "    best_f1 = -1.0\n",
        "    best_path = os.path.join(out_dir, \"best.pt\")\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        losses = []\n",
        "        for xb, yb in dl_tr:\n",
        "            xb = xb.to(device, non_blocking=True); yb = yb.to(device, non_blocking=True)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
        "                logits = model(xb)\n",
        "                loss = crit(logits, yb)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt); scaler.update()\n",
        "            losses.append(loss.item())\n",
        "        sch.step()\n",
        "\n",
        "        va_acc, va_f1, va_cm, _ = evaluate(model, dl_va, device)\n",
        "        print(f\"Epoch {ep:02d} | loss={np.mean(losses):.4f} | val_acc={va_acc:.4f} | val_f1={va_f1:.4f}\")\n",
        "        print(\"Val CM:\\n\", va_cm)\n",
        "\n",
        "        if va_f1 > best_f1:\n",
        "            best_f1 = va_f1\n",
        "            torch.save({\"model\": model.state_dict()}, best_path)\n",
        "\n",
        "    print(\"Best macro-F1:\", best_f1)\n",
        "    print(\"Saved:\", best_path)\n",
        "    return best_path\n",
        "\n",
        "best_ckpt = train_resnet18(epochs=20, lr=3e-4, batch_size=128)\n",
        "best_ckpt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "buNInUGOtRaI",
        "outputId": "6c5afaae-ae64-4287-eab5-ae0461768a0a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "Train samples (crops): 112906 | Val: 52126\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3240857862.py:38: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n",
            "/tmp/ipython-input-3240857862.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
            "/tmp/ipython-input-3240857862.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 01 | loss=0.2446 | val_acc=0.8568 | val_f1=0.8587\n",
            "Val CM:\n",
            " [[16890   640    42]\n",
            " [ 5109 12302    61]\n",
            " [ 1532    83 15467]]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3240857862.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
            "/tmp/ipython-input-3240857862.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 02 | loss=0.2051 | val_acc=0.8489 | val_f1=0.8500\n",
            "Val CM:\n",
            " [[16617   632   323]\n",
            " [ 5492 11925    55]\n",
            " [ 1275   101 15706]]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3240857862.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
            "/tmp/ipython-input-3240857862.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 03 | loss=0.1969 | val_acc=0.8580 | val_f1=0.8606\n",
            "Val CM:\n",
            " [[17004   540    28]\n",
            " [ 4864 12588    20]\n",
            " [ 1863    89 15130]]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3240857862.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
            "/tmp/ipython-input-3240857862.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 04 | loss=0.1916 | val_acc=0.8606 | val_f1=0.8627\n",
            "Val CM:\n",
            " [[16295  1039   238]\n",
            " [ 4548 12896    28]\n",
            " [ 1294   117 15671]]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3240857862.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
            "/tmp/ipython-input-3240857862.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 05 | loss=0.1870 | val_acc=0.8442 | val_f1=0.8462\n",
            "Val CM:\n",
            " [[16240  1060   272]\n",
            " [ 5263 12187    22]\n",
            " [ 1383   122 15577]]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3240857862.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
            "/tmp/ipython-input-3240857862.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 06 | loss=0.1834 | val_acc=0.8702 | val_f1=0.8721\n",
            "Val CM:\n",
            " [[16384   847   341]\n",
            " [ 3926 13387   159]\n",
            " [ 1437    57 15588]]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3240857862.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
            "/tmp/ipython-input-3240857862.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 07 | loss=0.1827 | val_acc=0.8671 | val_f1=0.8692\n",
            "Val CM:\n",
            " [[16739   767    66]\n",
            " [ 4612 12841    19]\n",
            " [ 1386    75 15621]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3240857862.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
            "/tmp/ipython-input-3240857862.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 08 | loss=0.1777 | val_acc=0.8699 | val_f1=0.8722\n",
            "Val CM:\n",
            " [[16481  1060    31]\n",
            " [ 4258 13193    21]\n",
            " [ 1352    61 15669]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3240857862.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
            "/tmp/ipython-input-3240857862.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 09 | loss=0.1749 | val_acc=0.8679 | val_f1=0.8696\n",
            "Val CM:\n",
            " [[16737   677   158]\n",
            " [ 4687 12739    46]\n",
            " [ 1266    50 15766]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3240857862.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
            "/tmp/ipython-input-3240857862.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 | loss=0.1750 | val_acc=0.8734 | val_f1=0.8757\n",
            "Val CM:\n",
            " [[16354  1146    72]\n",
            " [ 3974 13482    16]\n",
            " [ 1327    65 15690]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3240857862.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
            "/tmp/ipython-input-3240857862.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 | loss=0.1731 | val_acc=0.8683 | val_f1=0.8705\n",
            "Val CM:\n",
            " [[16574   949    49]\n",
            " [ 4418 13041    13]\n",
            " [ 1373    64 15645]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3240857862.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
            "/tmp/ipython-input-3240857862.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 | loss=0.1723 | val_acc=0.8656 | val_f1=0.8678\n",
            "Val CM:\n",
            " [[16670   879    23]\n",
            " [ 4629 12837     6]\n",
            " [ 1398    70 15614]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3240857862.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
            "/tmp/ipython-input-3240857862.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 | loss=0.1711 | val_acc=0.8687 | val_f1=0.8707\n",
            "Val CM:\n",
            " [[16611   863    98]\n",
            " [ 4514 12942    16]\n",
            " [ 1281    73 15728]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3240857862.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
            "/tmp/ipython-input-3240857862.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 | loss=0.1707 | val_acc=0.8665 | val_f1=0.8687\n",
            "Val CM:\n",
            " [[16473  1037    62]\n",
            " [ 4442 13021     9]\n",
            " [ 1332    78 15672]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3240857862.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
            "/tmp/ipython-input-3240857862.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 | loss=0.1705 | val_acc=0.8624 | val_f1=0.8645\n",
            "Val CM:\n",
            " [[16560   934    78]\n",
            " [ 4765 12702     5]\n",
            " [ 1318    70 15694]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3240857862.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
            "/tmp/ipython-input-3240857862.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 | loss=0.1704 | val_acc=0.8598 | val_f1=0.8617\n",
            "Val CM:\n",
            " [[16739   787    46]\n",
            " [ 5048 12412    12]\n",
            " [ 1367    48 15667]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3240857862.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
            "/tmp/ipython-input-3240857862.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 | loss=0.1702 | val_acc=0.8601 | val_f1=0.8624\n",
            "Val CM:\n",
            " [[16532  1011    29]\n",
            " [ 4791 12676     5]\n",
            " [ 1386    69 15627]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3240857862.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
            "/tmp/ipython-input-3240857862.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 | loss=0.1701 | val_acc=0.8646 | val_f1=0.8669\n",
            "Val CM:\n",
            " [[16564   978    30]\n",
            " [ 4609 12858     5]\n",
            " [ 1369    65 15648]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3240857862.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
            "/tmp/ipython-input-3240857862.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 | loss=0.1700 | val_acc=0.8630 | val_f1=0.8652\n",
            "Val CM:\n",
            " [[16630   903    39]\n",
            " [ 4766 12700     6]\n",
            " [ 1351    74 15657]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3240857862.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
            "/tmp/ipython-input-3240857862.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 | loss=0.1700 | val_acc=0.8642 | val_f1=0.8664\n",
            "Val CM:\n",
            " [[16669   873    30]\n",
            " [ 4720 12748     4]\n",
            " [ 1376    76 15630]]\n",
            "Best macro-F1: 0.8756921285880935\n",
            "Saved: /content/drive/MyDrive/kaggle/working/synthdata/runs_resnet18_yolocrops/best.pt\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/kaggle/working/synthdata/runs_resnet18_yolocrops/best.pt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation Results (Epoch 20)\n",
        "\n",
        "- **Val Accuracy:** `0.8642`\n",
        "- **Val macro-F1:** `0.8664`\n",
        "- **Best macro-F1 across training:** `0.8757` (saved as best checkpoint)\n",
        "- **Confusion Matrix (rows = true, cols = pred):**\n",
        "\n",
        "\n",
        "### Per-class Precision / Recall\n",
        "| Class      | Precision | Recall |\n",
        "|------------|-----------|--------|\n",
        "| Normal (0) | 0.732     | 0.949  |\n",
        "| Reversal(1)| 0.931     | 0.730  |\n",
        "| Corrected(2)| 0.998    | 0.915  |\n",
        "\n",
        "> Computed from the confusion matrix. High precision for **Reversal** but lower recall indicates many true Reversal samples are predicted as **Normal**.\n",
        "\n",
        "## Error Analysis (from CM)\n",
        "- **Reversal → Normal** is the dominant confusion (`4720` cases), suggesting the model under-detects reversal cues when visual evidence is subtle or cropped too tightly.\n",
        "- **Normal ↔ Corrected** confusion is minimal, indicating the model reliably distinguishes corrected glyphs once visible.\n",
        "\n",
        "## Takeaways & Next Steps\n",
        "- **Context preservation:** Slightly **inflate boxes** (e.g., `+10%`) during training crops to retain local context that signals reversals.\n",
        "- **Class balance:** Keep `WeightedRandomSampler` (or add class-weighted loss / focal loss) to further improve recall on **Reversal**.\n",
        "- **Light appearance jitter:** A small `ColorJitter(brightness=0.05, contrast=0.05)` (no flips/rotations) can improve robustness to scan/lighting variations without breaking dyslexia-relevant geometry.\n",
        "- **Capacity/regularization sweep:** Try EfficientNet-B0 or MobileNetV3 as baselines; monitor macro-F1 and specifically **Reversal recall**.\n"
      ],
      "metadata": {
        "id": "eR5Pj7uEJW8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_report(ckpt_path):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    _, ds_va, _, dl_va = make_loaders(ROOT, batch_size=128, num_workers=2)\n",
        "    model = build_resnet18(NUM_CLASSES, pretrained=False).to(device)\n",
        "    sd = torch.load(ckpt_path, map_location=\"cpu\")[\"model\"]\n",
        "    model.load_state_dict(sd)\n",
        "    acc, f1, cm, rep = evaluate(model, dl_va, device)\n",
        "    print(\"VAL accuracy:\", f\"{acc:.4f}\")\n",
        "    print(\"VAL macro-F1:\", f\"{f1:.4f}\")\n",
        "    print(\"Confusion Matrix:\\n\", cm)\n",
        "    print(rep)\n",
        "\n",
        "load_and_report(best_ckpt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-l2f-hYQtVwH",
        "outputId": "e2766a17-943c-4a95-833a-d016256bd9f1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3240857862.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAL accuracy: 0.8734\n",
            "VAL macro-F1: 0.8757\n",
            "Confusion Matrix:\n",
            " [[16354  1146    72]\n",
            " [ 3974 13482    16]\n",
            " [ 1327    65 15690]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.76      0.93      0.83     17572\n",
            "    Reversal       0.92      0.77      0.84     17472\n",
            "   Corrected       0.99      0.92      0.95     17082\n",
            "\n",
            "    accuracy                           0.87     52126\n",
            "   macro avg       0.89      0.87      0.88     52126\n",
            "weighted avg       0.89      0.87      0.88     52126\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What this cell does\n",
        "- Rebuilds the validation dataloader and **reconstructs the model** with the same architecture as during training.\n",
        "- **Loads the saved checkpoint** (`best_ckpt`) into the model (using `map_location=\"cpu\"` for portability).\n",
        "- Runs the shared `evaluate(...)` routine to print **Validation Accuracy**, **macro-F1**, **Confusion Matrix**, and a full **classification report**.\n",
        "\n",
        "## Why it matters\n",
        "- Separates evaluation from training to **avoid accidental state leakage** (e.g., `model.train()` flags, stochastic layers).\n",
        "- Guarantees that the saved weights **deserialize correctly** into the intended architecture (sanity check for future reuse/deployment).\n",
        "- Produces stable, comparable metrics for model selection across different runs or model variants.\n",
        "\n",
        "## Validation Results\n",
        "- **VAL accuracy:** `0.8734`  \n",
        "- **VAL macro-F1:** `0.8757`  \n",
        "- **Confusion Matrix (rows = true, cols = pred):**\n",
        "\n",
        "[16354 1146 72]\n",
        "[ 3974 13482 16]\n",
        "[ 1327 65 15690]\n",
        "\n",
        "\n",
        "### Quick error analysis\n",
        "- The dominant confusion is **Reversal → Normal** (`3974` cases). This indicates the model **under-calls reversals** when cues are subtle or the crop is too tight.\n",
        "- **Corrected → Normal** (`1327`) is the next notable confusion; still much smaller than Reversal→Normal.\n",
        "- Column totals suggest a **bias toward predicting “Normal”** (predicted Normal = 21,655 vs. true Normal = 17,572), while **Reversal** is under-predicted.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fBCtGh0-Ki13"
      }
    }
  ]
}